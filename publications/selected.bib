@article{ahuja2019to,
  title={To React or not to React: End-to-End Visual Pose Forecasting for Personalized Avatar during Dyadic Conversations},
  author={Ahuja, Chaitanya and Ma, Shugao and Morency, Louis-Philippe and Sheikh, Yaser},
  year={2019},
  journal={ICMI},
  webpage={http://chahuja.com/trontr},
  _venue={ICMI},
  abstract={}
}
@article{ahuja2019language,
  title={Language2Pose: Natural Language Grounded Pose Forecasting},
  author={Ahuja, Chaitanya and Morency, Louis-Philippe},
  year={2019},
  journal={3DV},
  url={https://arxiv.org/pdf/1907.01108.pdf},
  codeurl={https://github.com/chahuja/language2pose},
  webpage={http://chahuja.com/language2pose},
  _venue={3DV},
  abstract={Generating animations from natural language sentences finds its applications in a a number of domains such as movie script visualization, virtual human animation and, robot motion planning. These sentences can describe different kinds of actions, speeds and direction of these actions, and possibly a target destination. The core modeling challenge in this language-to-pose application is how to map linguistic concepts to motion animations. In this paper, we address this multimodal problem by introducing a neural architecture called Joint Language-toPose (or JL2P), which learns a joint embedding of language and pose. This joint embedding space is learned end-toend using a curriculum learning approach which emphasizes shorter and easier sequences first before moving to longer and harder ones. We evaluate our proposed model on a publicly available corpus of 3D pose data and humanannotated sentences. Both objective metrics and human judgment evaluation confirm that our proposed approach is able to generate more accurate animations and are deemed visually more representative by humans than other data driven approaches.}
}

@article{ahuja2018lattice,
  title={Lattice Recurrent Unit: Improving Convergence and Statistical Efficiency for Sequence Modeling},
  author={Ahuja, Chaitanya and Morency, Louis-Philippe},
  journal={AAAI},
  year={2018},
  url={https://arxiv.org/pdf/1710.02254.pdf},
  codeurl={https://github.com/chahuja/lru},
  webpage={http://chahuja.com/lru},
  [presentation]_url={files/ppt_lru_speakingreq.pdf},
  _venue={AAAI},
  abstract={Recurrent neural networks have shown remarkable success in modeling sequences. However low resource situations still adversely affect the generalizability of these models. We introduce a new family of models, called Lattice Recurrent Units (LRU), to address the challenge of learning deep multi-layer recurrent models with limited resources. LRU models achieve this goal by creating distinct (but coupled) flow of information inside the units: a first flow along time dimension and a second flow along depth dimension. It also offers a symmetry in how information can flow horizontally and vertically. We analyze the effects of decoupling three different components of our LRU model: Reset Gate, Update Gate and Projected State. We evaluate this family of new LRU models on computational convergence rates and statistical efficiency. Our experiments are performed on four publicly-available datasets, comparing with Grid-LSTM and Recurrent Highway networks. Our results show that LRU has better empirical computational convergence rates and statistical efficiency values, along with learning more accurate language models.}
}

@article{baltruvsaitis2017multimodal,
  title={Multimodal Machine Learning: A Survey and Taxonomy},
  author={Baltrusaitis, Tadas and Ahuja, Chaitanya and Morency, Louis-Philippe},
  journal={TPAMI},
  year={2017},
  url={https://arxiv.org/abs/1705.09406},
  _venue={TPAMI},
  abstract={â€”Our experience of the world is multimodal - we see objects, hear sounds, feel texture, smell odors, and taste flavors. Modality refers to the way in which something happens or is experienced and a research problem is characterized as multimodal when it includes multiple such modalities. In order for Artificial Intelligence to make progress in understanding the world around us, it needs to be able to interpret such multimodal signals together. Multimodal machine learning aims to build models that can process and relate information from multiple modalities. It is a vibrant multi-disciplinary field of increasing importance and with extraordinary potential. Instead of focusing on specific multimodal applications, this paper surveys the recent advances in multimodal machine learning itself and presents them in a common taxonomy. We go beyond the typical early and late fusion categorization and identify broader challenges that are faced by multimodal machine learning, namely: representation, translation, alignment, fusion, and co-learning. This new taxonomy will enable researchers to better understand the state of the field and identify directions for future research.}
}

@inproceedings{ahuja2014fast,
  title={Fast modelling of pinna spectral notches from HRTFs using linear prediction residual cepstrum},
  author={Ahuja, Chaitanya and Hegde, Rajesh M},
  booktitle={IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  pages={4458--4462},
  year={2014},
  organization={IEEE},
  url={files/icassp_chahuja_paper.pdf},
  _venue={ICASSP},
  abstract={Developing individualized head related transfer functions (HRTF) is an essential requirement for accurate virtualization of sound. However it is time consuming and complicated for both the subject and the developer. Obtaining the spectral notches which are the most prominent features of HRTF is very important to reconstruct the head related impulse response (HRIR) accurately. In this paper, a method suitable for fast computation of the frequencies of spectral notches is proposed. The linear prediction residual cepstrum is used to compute the spectral notches with a high degree of accuracy in this work. Subsequent use of Batteaus Reflection model to overlay the spectral notches on the pinna images indicate that the proposed method is able to provide finer contours. Experiments on reconstruction of the HRIR indicates that the method performs better than other methods.}
}

@inproceedings{sohni2014extraction,
  title={Extraction of pinna spectral notches in the median plane of a virtual spherical microphone array},
  author={Sohni, Ankit and Ahuja, Chaitanya and Hegde, Rajesh M},
  booktitle={4th Joint Workshop on Hands-free Speech Communication and Microphone Arrays (HSCMA)},
  pages={142--146},
  year={2014},
  organization={IEEE},
  url={files/hscma_chahuja_paper.pdf},
  _venue={HSCMA},
  abstract={In this paper, a fast method for the extraction of pinna spectral notches (PSN) in the median plane of a virtual spherical microphone array is discussed. In general, PSN can be extracted from the Head Related Impulse Response (HRIR) measured by a spherical array of microphones. However, the PSN extracted herein are computationally complex and also not accurate at lower elevation angles. This work proposes a novel approach to reconstruct the HRIR using microphones over the median plane of a virtual spherical array. The virtual spherical array itself is simulated using the Fourier Bessel series (FBS). Subsequently, these HRIRs are used to extract the PSN. This method is computationally efficient since it is done over the median plane rather than over the complete sphere. On the other hand, it is also accurate due to the utilization of the Fourier Bessel series in the extraction of the PSN. Experimental results obtained on the CIPIC database indicate a high degree of resemblance to the actual pinna walls, even at the lower elevation angles. The results are motivating enough for the method to be considered for resolving elevation ambiguity in 3D audio.}
}